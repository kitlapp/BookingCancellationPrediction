{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11647605,"sourceType":"datasetVersion","datasetId":7309194},{"sourceId":11648663,"sourceType":"datasetVersion","datasetId":7309963}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kimonioannislappas/hotel-booking-predictive-models-kpi-dashboard?scriptVersionId=237433166\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Hotel Booking: From DataOps to Predictive Models & Business KPIs\n## Part 1: Exploratory Machine Learning for Booking Cancellations & Stakeholders Reporting\n## Part 2: KPI Business Dashboard in Looker Studio for Strategic Planning Monitoring","metadata":{}},{"cell_type":"markdown","source":"### All datasets and scripts are public — just click **\"Copy & Edit\"** to run this notebook yourself.","metadata":{}},{"cell_type":"markdown","source":"## **IMPORTANT INFORMATION BELOW**  \n***************************************************************************************************************************************************\nThe initial project structure was developed in PyCharm, as it is a great option for building automation workflows. I also designed a full case study to simulate realistic challenges that hotel booking stakeholders (hotel owners, management teams, directors) might face.\n\nThis Kaggle notebook is just a version of the same project, which is originally hosted on GitHub in two separate subprojects. I provide the GitHub links to the initial project below.\n\nI am especially happy with this Kaggle-adjusted version because it solves a key limitation I faced on GitHub: I couldn’t share my full working environment. The GitHub version is designed to connect to my local PostgreSQL database (to simulate a real-world setup), so the scripts can’t run out of the box on other machines. Here on Kaggle, though, I compromise real-world setup and just read the raw data into a pandas DataFrame (df) and hence, all you need to do is run this notebook!\n\n***************************************************************************************************************************************************\nAlthough the GitHub version isn't able to be easily shared, offers other advantages, particularly in terms of professionalism. The workflows are designed to closely simulate real-world scenarios and project structures. To explore the full project workflow, including Data and MLOps pipelines, as well as future upgrade suggestions, please refer to the documentation:    \n**[DOCUMENTATION](https://drive.google.com/file/d/1xB5_aj08BEMVliJLg-h41F0X0rYDWYoN/view?usp=sharing)**  \n***************************************************************************************************************************************************\nHere are the GitHub links to the initial projects:  \n[Part 1](https://github.com/kitlapp/BookingCancellationPrediction)  \n[Part 2](https://github.com/kitlapp/LookerStudioKpiDashboard)\n\nAnd here is the final Looker Studio KPI dashboard structure:  \n[Looker Studio KPI Dashboard](https://lookerstudio.google.com/reporting/8ee13cf9-54e6-41ac-823e-af0706cec66c)\n***************************************************************************************************************************************************\n**OTHER NOTES ABOUT MY INITIAL PROJECT:**  \nNote that I understand the weaknesses of my initial project, neither did I consider extreme cases or scalability when I had been developing it and this is the reason I include a future suggestions section in the provided documentation file. My solutions works though, at least at the level I assumed in my case study.\n\nAdditionally, I am completely self-taught in Data Science. Lacking jargon experience, I am questioning some Data Science senior operations terms. That is, I am happy with my DataOps but I already know that I could provide more MLOps and not only a log file for script execution monitoring. For this reason, I would prefer to use a more general language to describe what I did such as Data Science Operations – DSOps to be 100% to the point but I am afraid I will undersell my work if I would do that. Thus, I use the \"correct\" jargon terms but apriori state that especially project's MLOps has room for improvements.\n***************************************************************************************************************************************************","metadata":{}},{"cell_type":"markdown","source":"## Assumptions\n**1)** Both hotels have the same stakeholders (belong to the same group) who ask for answers about the high cancellation rates.  \n**2)** Since I can't reach the stakeholders for more information, I am free to logically adapt to any potential challenges in order to properly interpret the results.    \n***************************************************************************************************************************************************","metadata":{}},{"cell_type":"markdown","source":"## Part 1: Exploratory Machine Learning for Booking Cancellations & Stakeholders Reporting  \n\nThe goal is to predict the is_canceled feature following the owner's request to understand the reasons behind the high cancellation rates. Cancellations are a significant issue in the hospitality industry, as a room is reserved until the cancellation date, potentially resulting in lost revenue. If the cancellation occurs late, the room may not be rebooked in time. Furthermore, if the accommodation facility lacks a property management system (PMS), a high volume of cancellations can create additional complications and administrative overload. This increases operational costs by adding extra work for employees who do not contribute to the hotel's revenue-generating activities. Such inefficiencies are prone to errors, which could lead to double bookings or false reservations, further disrupting hotel operations and customer experience.","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries and Basics & Read Raw Data","metadata":{}},{"cell_type":"code","source":"# Copy the required script and data files to the working directory so they can be accessed by this notebook\nimport shutil\nshutil.copy(\"/kaggle/input/secondary-scripts/cleaning.py\", \"/kaggle/working/\"); \nshutil.copy(\"/kaggle/input/secondary-scripts/dictionaries.py\", \"/kaggle/working/\");\nshutil.copy(\"/kaggle/input/secondary-scripts/results.py\", \"/kaggle/working/\");\nshutil.copy(\"/kaggle/input/raw-data/hotel_booking.csv\", \"/kaggle/working/\");","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:16.58092Z","iopub.execute_input":"2025-05-02T15:46:16.581236Z","iopub.status.idle":"2025-05-02T15:46:16.677925Z","shell.execute_reply.started":"2025-05-02T15:46:16.581213Z","shell.execute_reply":"2025-05-02T15:46:16.676915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Built-in libraries\nimport os\nimport time\nimport warnings\n\n# Core data handling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical tools\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant     \n\n# Scikit-learn: preprocessing, model training, evaluation\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, \n    confusion_matrix, balanced_accuracy_score, matthews_corrcoef,\n)\n\n# Custom modules\nfrom cleaning import (\n    explore_outliers, \n    month_components_calculation, \n    day_components_calculation\n)\nfrom dictionaries import country_to_category\nfrom results import get_classification_metrics, logreg_interpretation, random_forest_interpretation\n\n# Display and warning options\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.display.max_columns = 999","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:16.679314Z","iopub.execute_input":"2025-05-02T15:46:16.679674Z","iopub.status.idle":"2025-05-02T15:46:16.687664Z","shell.execute_reply.started":"2025-05-02T15:46:16.679643Z","shell.execute_reply":"2025-05-02T15:46:16.686544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read raw dataset to a df\ndf_raw = pd.read_csv(\"hotel_booking.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:16.688995Z","iopub.execute_input":"2025-05-02T15:46:16.689365Z","iopub.status.idle":"2025-05-02T15:46:17.54677Z","shell.execute_reply.started":"2025-05-02T15:46:16.689335Z","shell.execute_reply":"2025-05-02T15:46:17.545724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Preprocessing  \nI handle the preprocessing steps of both projects at the same notebook (here). Note that the final cleaned data will be **logreg_rf_data.csv** and **dashboard_data.csv**. Since I need to follow different preprocessing steps depending on the objectives of each cleaned dataset, I note in the cells which preprocessing script is related to each cleaned dataset.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Handle NaNs","metadata":{}},{"cell_type":"code","source":"# Explore the NaNs:\ndf_raw.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:17.549377Z","iopub.execute_input":"2025-05-02T15:46:17.549667Z","iopub.status.idle":"2025-05-02T15:46:17.660954Z","shell.execute_reply.started":"2025-05-02T15:46:17.549647Z","shell.execute_reply":"2025-05-02T15:46:17.660215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create two datafiles for preprocessing:\n# One for ML tasks (df<number>) and a second one (dfdash<number>) for KPI dataframes creation\n# 'dash' is from 'dashboard' because it will be used for KPI dataframes creation for looker studio dashboard\n\ndf2 = df_raw.copy()\ndfdash = df_raw.copy()\n\n# Fill missing values in 'children' column with the most frequent value (mode)\ndf2['children'] = df2['children'].fillna(value=df2['children'].mode()[0])\n\n# Replace missing values in 'agent' with 0, indicating direct bookings without a travel agent\ndf2['agent'] = df2['agent'].fillna(value=0)\n\n# Replace missing values in 'company' with 0, meaning bookings not linked to any company\ndf2['company'] = df2['company'].fillna(value=0)\n\n# Drop all rows with missing 'country' values, since location info is important for analysis\ndf2 = df2.drop(labels=df2.loc[df2['country'].isna()].index)\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n # Fill missing values similarly for dashboard DataFrame\ndfdash['children'] = dfdash['children'].fillna(value=dfdash['children'].mode()[0])\ndfdash['agent'] = dfdash['agent'].fillna(value=0)\ndfdash['company'] = dfdash['company'].fillna(value=0)\ndfdash = dfdash.drop(labels=dfdash.loc[dfdash['country'].isna()].index)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:17.662025Z","iopub.execute_input":"2025-05-02T15:46:17.662296Z","iopub.status.idle":"2025-05-02T15:46:17.79957Z","shell.execute_reply.started":"2025-05-02T15:46:17.662271Z","shell.execute_reply":"2025-05-02T15:46:17.798769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2. Handle Date-Related Columns","metadata":{}},{"cell_type":"code","source":"# Make copies of the previous files\ndf3 = df2.copy()\ndfdash2 = dfdash.copy()\n\n# Create a dictionary to convert month names to their corresponding numeric values (as strings)\nmonth_mapping = {\n    \"January\": '1', \"February\": '2', \"March\": '3', \"April\": '4', \"May\": '5', \"June\": '6',\n    \"July\": '7', \"August\": '8', \"September\": '9', \"October\": '10', \"November\": '11', \"December\": '12'\n}\n\n# Map month names in 'arrival_date_month' column to their numeric equivalents and convert them to integers\ndf3['arrival_date_month'] = df3['arrival_date_month'].map(month_mapping).astype(int)\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# I create a completed date column just in case I need to make Looker Studio interact with dates\n\n# Map month names to integers using the same dictionary\ndfdash2['arrival_date_month'] = dfdash2['arrival_date_month'].map(month_mapping).astype(int)\n\n# Combine year, month, and day columns into a single date string in 'YYYY-MM-DD' format\ndfdash2['arrival_date'] = (\n    dfdash2['arrival_date_year'].astype(str) + '-' +\n    dfdash2['arrival_date_month'].astype(str) + '-' +\n    dfdash2['arrival_date_day_of_month'].astype(str)\n)\n\n# Convert the date strings into proper datetime objects for easier time-based analysis\ndfdash2['arrival_date'] = pd.to_datetime(dfdash2['arrival_date'], format='%Y-%m-%d')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:17.800356Z","iopub.execute_input":"2025-05-02T15:46:17.800669Z","iopub.status.idle":"2025-05-02T15:46:18.066776Z","shell.execute_reply.started":"2025-05-02T15:46:17.800647Z","shell.execute_reply":"2025-05-02T15:46:18.065356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# As expected, the month of arrival is very strongly correlated with the week number of arrival.\n# Let's confirm this by calculating their correlation:\ndf3['arrival_date_month'].corr(df3['arrival_date_week_number'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.067854Z","iopub.execute_input":"2025-05-02T15:46:18.068167Z","iopub.status.idle":"2025-05-02T15:46:18.080164Z","shell.execute_reply.started":"2025-05-02T15:46:18.068135Z","shell.execute_reply":"2025-05-02T15:46:18.079299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since they are highly correlated, we can safely drop 'arrival_date_week_number' to reduce redundancy.\ndf3 = df3.drop(columns='arrival_date_week_number')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.081907Z","iopub.execute_input":"2025-05-02T15:46:18.082362Z","iopub.status.idle":"2025-05-02T15:46:18.127181Z","shell.execute_reply.started":"2025-05-02T15:46:18.082327Z","shell.execute_reply":"2025-05-02T15:46:18.126067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Months and days of month have a cyclical nature which is not profound in the 1 to 12-month format and the corresponding 1 to 31-day format. The model will treat them as raw values where 12 is bigger than 1 and so on. To fix this, I created a custom function which calculates the cosine and sine components of months and days. The function can properly treat all possible number of days a month has, as well as February days even if it is a leap year.","metadata":{}},{"cell_type":"code","source":"# Apply custom functions to perform cyclic encoding on the 'arrival_date_month' and 'arrival_date_day_of_month' column.\n# This helps machine learning models better understand the cyclical nature of months and days of months.\ndf4 = month_components_calculation(\n    dataframe=df3, \n    month_columns=['arrival_date_month']\n)\ndf5 = day_components_calculation(\n    dataframe=df4, \n    year_columns=['arrival_date_year'], \n    month_columns=['arrival_date_month'], \n    day_columns=['arrival_date_day_of_month']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.128224Z","iopub.execute_input":"2025-05-02T15:46:18.128581Z","iopub.status.idle":"2025-05-02T15:46:18.26723Z","shell.execute_reply.started":"2025-05-02T15:46:18.128553Z","shell.execute_reply":"2025-05-02T15:46:18.26626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3. Check for Duplicates","metadata":{}},{"cell_type":"code","source":"# Count the number of fully duplicated rows in the preprocessed main dataframe\ndf5.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.270543Z","iopub.execute_input":"2025-05-02T15:46:18.270788Z","iopub.status.idle":"2025-05-02T15:46:18.484151Z","shell.execute_reply.started":"2025-05-02T15:46:18.270771Z","shell.execute_reply":"2025-05-02T15:46:18.483544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4. Drop some Columns: Data Leakage, High Cardinality, Data Anomalies & Strong Corellations  \nI drop high cardinality columns which are unimportant for predicting cancellations. Additionally, columns associated with reservations must be dropped since they carry future information. For instance, if the reservation status date is earlier than the arrival date, the booking had already been canceled. The assigned room type was also dropped for the same reason.  \n\nThere are some data anomalies in the deposit type and car parking columns. Specifically, 99% of people who chose a non-refundable deposit type canceled. Additionally, 100% of people who required a parking space didn’t cancel. These are very strange, and since I can't reach the stakeholders for further clarification, they can’t be relied upon. For this reason, I dropped them from the dataset.  \n\nFinally, the week number is highly correlated with the month of arrival, which makes sense. I decided to drop the week number and keep the month of arrival.","metadata":{}},{"cell_type":"code","source":"# Make copies of the previous files\ndf6 = df5.copy()\ndfdash3 = dfdash2.copy()\n\n# List of columns to be dropped as they are not needed for analysis\ncols_to_be_dropped = ['name', 'email', 'arrival_date_month', 'arrival_date_day_of_month', 'phone-number', 'credit_card', 'reservation_status', \n                      'reservation_status_date', 'assigned_room_type','deposit_type', 'required_car_parking_spaces']\n\n# Drop the columns specified in 'cols_to_be_dropped'\ndf6 = df6.drop(columns= cols_to_be_dropped)\n\n# Display the shape of the dataframe after dropping columns\ndf6.shape\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# List of columns to be dropped from the dashboard dataframe\ndashcols_to_be_dropped = ['name', 'email', 'arrival_date_month', 'arrival_date_day_of_month', 'phone-number', 'credit_card', 'reservation_status', \n                      'reservation_status_date', 'assigned_room_type','deposit_type', 'required_car_parking_spaces', 'arrival_date_week_number']\n\n# Drop the unimportant columns from the dashboard dataframe\ndfdash3 = dfdash3.drop(columns= dashcols_to_be_dropped)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.485132Z","iopub.execute_input":"2025-05-02T15:46:18.485394Z","iopub.status.idle":"2025-05-02T15:46:18.687748Z","shell.execute_reply.started":"2025-05-02T15:46:18.485373Z","shell.execute_reply":"2025-05-02T15:46:18.686834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.5. Create total_kids Column","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf7 = df6.copy()\ndfdash4 = dfdash3.copy()\n\n# Merge the 'children' and 'babies' columns to create a new column 'total_kids' representing the total number of kids\ndf7['total_kids'] = df7['children'].astype(int) + df7['babies'].astype(int)\n\n# Drop the original 'children' and 'babies' columns after merging\ndf7 = df7.drop(columns=['children', 'babies'])\n\n# Drop rows with outliers (total kids > 3) and reset index\ndf7 = df7.loc[df7['total_kids'] <= 3].reset_index(drop=True)\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Merge the 'children' and 'babies' columns to create a new column 'total_kids' representing the total number of kids\ndfdash4['total_kids'] = dfdash4['children'].astype(int) + dfdash4['babies'].astype(int)\n\n# Drop the original 'children' and 'babies' columns after merging\ndfdash4 = dfdash4.drop(columns=['children', 'babies'])\n\n# Drop rows with outliers (total kids > 3) and reset index in the dashboard dataframe\ndfdash4 = dfdash4.loc[dfdash4['total_kids'] <= 3].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.688769Z","iopub.execute_input":"2025-05-02T15:46:18.689089Z","iopub.status.idle":"2025-05-02T15:46:18.830022Z","shell.execute_reply.started":"2025-05-02T15:46:18.689058Z","shell.execute_reply":"2025-05-02T15:46:18.829065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6. Handle adults Column","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf8 = df7.copy()\ndfdash5 = dfdash4.copy()\n\n# Exclude bookings where the number of adults is 0. Also, ensure that the number of adults is between 1 and 4.\ndf8 = df8[(df8['adults'] > 0) & (df8['adults'] <= 4)].reset_index(drop=True)\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Exclude bookings where the number of adults is 0. Also, ensure that the number of adults is between 1 and 4.\ndfdash5 = dfdash5[(dfdash5['adults'] > 0) & (dfdash5['adults'] <= 4)].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.831234Z","iopub.execute_input":"2025-05-02T15:46:18.831856Z","iopub.status.idle":"2025-05-02T15:46:18.924979Z","shell.execute_reply.started":"2025-05-02T15:46:18.831823Z","shell.execute_reply":"2025-05-02T15:46:18.924174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.7. Handle meal Column","metadata":{}},{"cell_type":"code","source":"df9 = df8.copy()\ndfdash6 = dfdash5.copy()\n\n# Drop rows where the 'meal' column is 'Undefined', indicating no meal choice\ndf9 = df9.drop(labels=df9[df9['meal'] == 'Undefined'].index).reset_index(drop=True)\n\n# Rename the 'meal' column to 'number_of_meals' for clarity\ndf9 = df9.rename(columns={'meal': 'number_of_meals'})\n\n# Create a dictionary to map meal types to numerical values\nmeal_mapping = {'BB': 1, 'HB': 2, 'SC': 0, 'FB': 3}\n\n# Map the dictionary to the 'number_of_meals' column, reducing complexity\ndf9['number_of_meals'] = df9['number_of_meals'].map(meal_mapping).astype(int)\n\n# *** Ultimately, the 'meal' feature was reduced from 5 categories to 3 categories! ***\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Drop rows where the 'meal' column is 'Undefined', indicating no meal choice\ndfdash6 = dfdash6.drop(labels=dfdash6[dfdash6['meal'] == 'Undefined'].index).reset_index(drop=True)\n\n# Rename the 'meal' column to 'number_of_meals' for clarity\ndfdash6 = dfdash6.rename(columns={'meal': 'number_of_meals'})\n\n# Map the dictionary to the 'number_of_meals' column, reducing complexity\ndfdash6['number_of_meals'] = dfdash6['number_of_meals'].map(meal_mapping).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:18.926012Z","iopub.execute_input":"2025-05-02T15:46:18.926337Z","iopub.status.idle":"2025-05-02T15:46:19.093983Z","shell.execute_reply.started":"2025-05-02T15:46:18.92631Z","shell.execute_reply":"2025-05-02T15:46:19.092882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.8. Handle country Column","metadata":{}},{"cell_type":"code","source":"# Make copy of previous file\ndf10 = df9.copy()\n\n# Map the 'country' column values to a smaller set of categories using the 'country_to_category' dictionary.\n# This reduces 177 unique country values to only 15.\ndf10['country'] = df10['country'].map(country_to_category).astype('category')\n\n# Drop the rows where the 'country' column is 'Antarctica', as it represents very few bookings and will reduce model complexity.\ndf10 = df10.drop(labels=df10[df10['country'] == 'Antarctica'].index, axis=0).reset_index(drop=True)\n\n# Remove 'Antarctica' from the category list, as it has been dropped.\ndf10['country'] = df10['country'].cat.remove_categories('Antarctica')\n\n# Drop any rows with NaN values to ensure clean data.\ndf10 = df10.dropna()\n\n# *** Ultimately, the 'country' feature was reduced from 177 categories to only 15 categories! ***","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:19.094851Z","iopub.execute_input":"2025-05-02T15:46:19.095085Z","iopub.status.idle":"2025-05-02T15:46:19.314426Z","shell.execute_reply.started":"2025-05-02T15:46:19.095068Z","shell.execute_reply":"2025-05-02T15:46:19.313138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.9. Handle market_segment Column","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf11 = df10.copy()\ndfdash7 = dfdash6.copy()\n\n# Drop all rows where the 'market_segment' column has the category 'Undefined', as it includes very few observations\ndf11 = df11.drop(labels=df11[df11['market_segment'] == 'Undefined'].index).reset_index(drop=True)\n\n# Replace the 'Complementary' and 'Aviation' categories in the 'market_segment' column with 'Other' to consolidate rare categories\ndf11['market_segment'] = df11['market_segment'].replace({'Complementary': 'Other', 'Aviation': 'Other'}).astype('category')\n\n# *** Ultimately, the 'market_segment' feature was reduced from 8 to 5 categories! ***\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Drop all rows where the 'market_segment' column has the category 'Undefined'\ndfdash7 = dfdash7.drop(labels=dfdash7[dfdash7['market_segment'] == 'Undefined'].index).reset_index(drop=True)\n\n# Replace the 'Complementary' and 'Aviation' categories in the 'market_segment' column with 'Other'\ndfdash7['market_segment'] = dfdash7['market_segment'].replace({'Complementary': 'Other', 'Aviation': 'Other'}).astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:19.318398Z","iopub.execute_input":"2025-05-02T15:46:19.323611Z","iopub.status.idle":"2025-05-02T15:46:19.639721Z","shell.execute_reply.started":"2025-05-02T15:46:19.323036Z","shell.execute_reply":"2025-05-02T15:46:19.638844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.10. Handle distribution_channel Column","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf12 = df11.copy()\ndfdash8 = dfdash7.copy()\n\n# Drop all rows where the 'distribution_channel' column has the category 'Undefined', as it includes very few observations\ndf12 = df12.drop(labels=df12[df12['distribution_channel'] == 'Undefined'].index).reset_index(drop=True)\n\n# Convert the 'distribution_channel' column to categorical type\ndf12['distribution_channel'] = df12['distribution_channel'].astype('category')\n\n# *** Ultimately, the 'distribution_channel' feature was reduced from 5 to 3 categories! ***\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Drop all rows where the 'distribution_channel' column has the category 'Undefined'\ndfdash8 = dfdash8.drop(labels=dfdash8[dfdash8['distribution_channel'] == 'Undefined'].index).reset_index(drop=True)\n\n# Convert the 'distribution_channel' column to categorical type\ndfdash8['distribution_channel'] = dfdash8['distribution_channel'].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:19.640827Z","iopub.execute_input":"2025-05-02T15:46:19.641348Z","iopub.status.idle":"2025-05-02T15:46:19.750333Z","shell.execute_reply.started":"2025-05-02T15:46:19.641316Z","shell.execute_reply":"2025-05-02T15:46:19.749584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.11. Handle reserved_room_type Column","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf13 = df12.copy()\ndfdash9 = dfdash8.copy()\n\n# Merge categories in the 'reserved_room_type' column, combining multiple categories into 'Other'\ndf13['reserved_room_type'] = df13['reserved_room_type'].replace({'C': 'Other', 'B': 'Other', 'H': 'Other', 'L': 'Other'}).astype('category')\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Merge categories in the 'reserved_room_type' column, combining multiple categories into 'Other'\ndfdash9['reserved_room_type'] = dfdash9['reserved_room_type'].replace({'C': 'Other', 'B': 'Other', 'H': 'Other', 'L': 'Other'}).astype('category')\n\n# *** Ultimately, the 'reserved_room_type' feature was reduced from 9 to 6 categories! ***","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:19.751149Z","iopub.execute_input":"2025-05-02T15:46:19.751376Z","iopub.status.idle":"2025-05-02T15:46:19.852641Z","shell.execute_reply.started":"2025-05-02T15:46:19.751359Z","shell.execute_reply":"2025-05-02T15:46:19.851535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.12. Handle agent & company Columns","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf14 = df13.copy()\ndfdash10 = dfdash9.copy()\n\n# Convert 'agent' column to binary: 1 if not 0, else 0\ndf14['agent'] = df14['agent'].apply(lambda x: 1 if x != 0 else 0)\n\n# Convert 'company' column to binary: 1 if not 0, else 0\ndf14['company'] = df14['company'].apply(lambda x: 1 if x != 0 else 0)\n\n# Rename the columns to more intuitive names\ndf14 = df14.rename(columns={'agent': 'has_agent', 'company': 'has_company'})\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Convert 'agent' column to binary: 1 if not 0, else 0\ndfdash10['agent'] = dfdash10['agent'].apply(lambda x: 1 if x != 0 else 0)\n\n# Convert 'company' column to binary: 1 if not 0, else 0\ndfdash10['company'] = dfdash10['company'].apply(lambda x: 1 if x != 0 else 0)\n\n# Rename the columns to more intuitive names\ndfdash10 = dfdash10.rename(columns={'agent': 'has_agent', 'company': 'has_company'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:19.853454Z","iopub.execute_input":"2025-05-02T15:46:19.853714Z","iopub.status.idle":"2025-05-02T15:46:20.121179Z","shell.execute_reply.started":"2025-05-02T15:46:19.853695Z","shell.execute_reply":"2025-05-02T15:46:20.120035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.13. Handle Cancellation Columns","metadata":{}},{"cell_type":"code","source":"# Make a copy of previous file\ndf15 = df14.copy()\n\n# Apply transformation to 'previous_cancellations' column:\n# Set to 2 if greater than or equal to 2, 0 if less than 1, else leave as is\ndf15['previous_cancellations'] = df15['previous_cancellations'].apply(lambda x: 2 if x >= 2 else (0 if x < 1 else x))\n\n# Apply transformation to 'previous_bookings_not_canceled' column:\n# Set to 2 if greater than or equal to 2, 0 if less than 1, else leave as is\ndf15['previous_bookings_not_canceled'] = df15['previous_bookings_not_canceled'].apply(lambda x: 2 if x >= 2 else (0 if x < 1 else x))\n\n# Rename the columns to more intuitive names\ndf15 = df15.rename(columns={'previous_cancellations': 'number_of_previous_cancellations', \n                            'previous_bookings_not_canceled': 'number_of_previous_bookings_not_canceled'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:20.122022Z","iopub.execute_input":"2025-05-02T15:46:20.122268Z","iopub.status.idle":"2025-05-02T15:46:20.237756Z","shell.execute_reply.started":"2025-05-02T15:46:20.122251Z","shell.execute_reply":"2025-05-02T15:46:20.236927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.14. Handle booking_changes & total_of_special_requests Columns","metadata":{}},{"cell_type":"code","source":"# Make a copy of previous file\ndf16 = df15.copy()\n\n# Apply transformation to 'booking_changes' column:\n# Set to 3 if greater than 2, 2 if equal to 2, 1 if equal to 1, else leave as is\ndf16['booking_changes'] = df16['booking_changes'].apply(lambda x: 3 if x > 2 else (2 if x == 2 else (1 if x == 1 else x)))\n\n# Apply transformation to 'total_of_special_requests' column:\n# Set to 3 if greater than 2, 2 if equal to 2, 1 if equal to 1, else leave as is\ndf16['total_of_special_requests'] = df16['total_of_special_requests'].apply(lambda x: 3 if x > 2 else (2 if x == 2 else (1 if x == 1 else x)))\n\n# Rename the columns to more intuitive names\ndf16 = df16.rename(columns={'booking_changes': 'number_of_booking_changes', 'total_of_special_requests': 'number_of_special_requests'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:20.238524Z","iopub.execute_input":"2025-05-02T15:46:20.238789Z","iopub.status.idle":"2025-05-02T15:46:20.3529Z","shell.execute_reply.started":"2025-05-02T15:46:20.238769Z","shell.execute_reply":"2025-05-02T15:46:20.351876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.15. Handle days_in_waiting_list Column","metadata":{}},{"cell_type":"code","source":"# Make a copy of previous file\ndf17 = df16.copy()\n\n# Apply transformation to 'days_in_waiting_list' column:\n# Set to 1 if greater than 0, else leave as is\ndf17['days_in_waiting_list'] = df17['days_in_waiting_list'].apply(lambda x: 1 if x > 0 else x)\n\n# Rename the 'days_in_waiting_list' column to 'has_waited'\ndf17 = df17.rename(columns={'days_in_waiting_list': 'has_waited'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:20.353904Z","iopub.execute_input":"2025-05-02T15:46:20.354203Z","iopub.status.idle":"2025-05-02T15:46:20.42038Z","shell.execute_reply.started":"2025-05-02T15:46:20.354177Z","shell.execute_reply":"2025-05-02T15:46:20.419549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.16. Handle Outliers","metadata":{}},{"cell_type":"code","source":"# Calling the explore_outliers function to visualize the distribution of some features\nexplore_outliers(dataframe=df17, column='lead_time', number_of_bins=60, negative=False);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:20.421293Z","iopub.execute_input":"2025-05-02T15:46:20.421545Z","iopub.status.idle":"2025-05-02T15:46:21.186576Z","shell.execute_reply.started":"2025-05-02T15:46:20.421526Z","shell.execute_reply":"2025-05-02T15:46:21.185265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make copies of previous files\ndf18 = df17.copy()\ndfdash11 = dfdash10.copy()\n\n# Set the threshold values for ADR (average daily rate) and lead_time outliers:\nadr_outlier_value = 5400  # Maximum acceptable value for ADR\nlead_time_outlier_border = 640  # Maximum acceptable value for lead time\n\n# Remove rows where ADR is higher than the defined threshold or negative:\ndf18 = df18.loc[(df18['adr'] < adr_outlier_value) & (df18['adr'] >= 0)].reset_index(drop=True)\n\n# Remove rows where lead_time exceeds the defined threshold:\ndf18 = df18.loc[df18['lead_time'] < lead_time_outlier_border].reset_index(drop=True)\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Remove rows where ADR is higher than the defined threshold or negative in the dashboard dataframe:\ndfdash11 = dfdash11.loc[(dfdash11['adr'] < adr_outlier_value) & (dfdash11['adr'] >= 0)].reset_index(drop=True)\n\n# Remove rows where lead_time exceeds the defined threshold in the dashboard dataframe:\ndfdash11 = dfdash11.loc[dfdash11['lead_time'] < lead_time_outlier_border].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:21.187452Z","iopub.execute_input":"2025-05-02T15:46:21.187708Z","iopub.status.idle":"2025-05-02T15:46:21.279864Z","shell.execute_reply.started":"2025-05-02T15:46:21.18769Z","shell.execute_reply":"2025-05-02T15:46:21.278985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.17. Final Check on dtypes","metadata":{}},{"cell_type":"code","source":"# Make copies of previous files\ndf19 = df18.copy()\ndfdash12 = dfdash11.copy()\n\n# Specify columns to be converted to categorical data type:\ncols_to_be_categorized = ['hotel', 'arrival_date_year', 'customer_type']\n\n# Convert the specified columns to categorical data type:\ndf19[cols_to_be_categorized] = df19[cols_to_be_categorized].astype('category')\n\n##### DASHBOARD DATAFRAME PREPROCESSING #####\n# Specify columns in the dashboard dataframe to be converted to categorical data type:\ndashcols_to_be_categorized = ['hotel', 'customer_type', 'country']\n\n# Convert the specified columns in the dashboard dataframe to categorical data type:\ndfdash12[dashcols_to_be_categorized] = dfdash12[dashcols_to_be_categorized].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:21.280721Z","iopub.execute_input":"2025-05-02T15:46:21.280965Z","iopub.status.idle":"2025-05-02T15:46:21.345341Z","shell.execute_reply.started":"2025-05-02T15:46:21.280946Z","shell.execute_reply":"2025-05-02T15:46:21.344548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.18. Encode Categories","metadata":{}},{"cell_type":"code","source":"# Specify the list of columns to be one-hot encoded:\ncategories = ['hotel', 'arrival_date_year', 'country', 'market_segment', 'distribution_channel', 'reserved_room_type', 'customer_type']\n\n# Apply one-hot encoding on the selected columns in df19 and drop the first category to avoid multicollinearity:\ndf20 = pd.get_dummies(data=df19, columns=categories, drop_first=True)\n\n# Identify columns with boolean data type:\nboolean_cols = df20.columns[df20.dtypes == 'bool']\n\n# Convert boolean columns to integers (True becomes 1, False becomes 0):\ndf20[boolean_cols] = df20[boolean_cols].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:21.346319Z","iopub.execute_input":"2025-05-02T15:46:21.346682Z","iopub.status.idle":"2025-05-02T15:46:21.408055Z","shell.execute_reply.started":"2025-05-02T15:46:21.346658Z","shell.execute_reply":"2025-05-02T15:46:21.407346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Check for Multicolinearity (Optional)","metadata":{}},{"cell_type":"code","source":"X = df20.copy()\n\n# Add a constant (intercept) column to the DataFrame X to use in regression models:\nX_with_const = add_constant(X)\n\n# Initialize an empty DataFrame to store the features and their corresponding VIF values:\nvif = pd.DataFrame()\n\n# Assign column names to the DataFrame: one for features and the other for VIF values:\nvif[\"Feature\"] = X_with_const.columns\n\n# Calculate the Variance Inflation Factor (VIF) for each feature in the dataset\n# and store the values in the \"VIF\" column of the DataFrame:\nvif[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n\n# Sort the DataFrame 'vif' in descending order based on the VIF values to identify \n# the features with the highest multicollinearity (i.e., those with the highest VIF).\nvif_sorted = vif.sort_values(by=\"VIF\", ascending=False)\n# Filter the sorted 'vif' DataFrame to display only the features with a VIF greater than 5\nvif_sorted[vif_sorted['VIF'] > 5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:21.409Z","iopub.execute_input":"2025-05-02T15:46:21.4093Z","iopub.status.idle":"2025-05-02T15:46:43.535332Z","shell.execute_reply.started":"2025-05-02T15:46:21.409273Z","shell.execute_reply":"2025-05-02T15:46:43.534522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Export Cleaned Files","metadata":{}},{"cell_type":"code","source":"df20.to_csv(\"logreg_rf_data.csv\", index=False)\ndfdash12.to_csv(\"dashboard_data.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:43.539319Z","iopub.execute_input":"2025-05-02T15:46:43.539607Z","iopub.status.idle":"2025-05-02T15:46:46.720556Z","shell.execute_reply.started":"2025-05-02T15:46:43.539588Z","shell.execute_reply":"2025-05-02T15:46:46.719583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Baseline Logistic Regression Model","metadata":{}},{"cell_type":"markdown","source":"The preprocessed file needed is the **logreg_rf_data.csv**.  \n***************************************************************************************************************************************************\n\nLogistic Regression runs smoothly without encountering any challenges. I decided not to use GridSearchCV because I don’t want to increase the runtime too much. I prefer to tune the hyperparameter values manually using trial-and-error techniques, reducing automation but conducting many trials in a short amount of time. The best logreg model occurres with the ‘newton-cg’ solver, which is efficient for large datasets with a lot of features.  \n\nAdditionally, I change class_weight to ‘balanced’ because my classes are slightly imbalanced and I don’t want to affect the dataset using either undersampling or oversampling techniques. Of course, I run the logreg model with random_state equal to 42 to compare its results with the Random Forest (from now on: ‘RF’) model.","metadata":{}},{"cell_type":"code","source":"# Read cleaned data for ML tasks\ndf = pd.read_csv(\"logreg_rf_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:46.721832Z","iopub.execute_input":"2025-05-02T15:46:46.722114Z","iopub.status.idle":"2025-05-02T15:46:47.135949Z","shell.execute_reply.started":"2025-05-02T15:46:46.722091Z","shell.execute_reply":"2025-05-02T15:46:47.135061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1. Create Features & Target","metadata":{}},{"cell_type":"code","source":"# Create the DataFrame with features:\nX = df.drop(columns=['is_canceled'], axis=1)\nprint('Shape of Features:', X.shape)\n# Create the target series:\ny = df['is_canceled']\nprint('Shape of Target:', y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:47.137008Z","iopub.execute_input":"2025-05-02T15:46:47.137292Z","iopub.status.idle":"2025-05-02T15:46:47.164649Z","shell.execute_reply.started":"2025-05-02T15:46:47.137267Z","shell.execute_reply":"2025-05-02T15:46:47.163867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2. Train Test Split","metadata":{}},{"cell_type":"code","source":"# Split features and target into train and test sets:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of y_train:', y_train.shape)\nprint('Shape of y_test:', y_test.shape)\nprint('Percentage of Minority Class in Training:', y_train.value_counts().values[1] * 100 / len(y_train))\nprint('Percentage of Minority Class in Test:', y_test.value_counts().values[1] * 100 / len(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:47.165393Z","iopub.execute_input":"2025-05-02T15:46:47.165698Z","iopub.status.idle":"2025-05-02T15:46:47.344284Z","shell.execute_reply.started":"2025-05-02T15:46:47.165677Z","shell.execute_reply":"2025-05-02T15:46:47.342738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3. Scale Features","metadata":{}},{"cell_type":"code","source":"features_to_scale = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'number_of_meals', 'number_of_previous_cancellations', \n                     'number_of_previous_bookings_not_canceled', 'number_of_booking_changes', 'adr', 'number_of_special_requests', 'total_kids']\n\n# Create the scaler instance:\nscaler = StandardScaler()\n\n# Scale the X_train set:\nX_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\nX_test[features_to_scale] = scaler.transform(X_test[features_to_scale])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:47.345703Z","iopub.execute_input":"2025-05-02T15:46:47.345967Z","iopub.status.idle":"2025-05-02T15:46:47.395464Z","shell.execute_reply.started":"2025-05-02T15:46:47.345947Z","shell.execute_reply":"2025-05-02T15:46:47.394252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.4. Train & Validate, Make Predictions and Display Evaluation Results","metadata":{}},{"cell_type":"code","source":"# Start the timer\nstart_time = time.time()\n\nmodel = LogisticRegression(penalty='l2', C=10, class_weight='balanced', random_state=42, solver='newton-cg', max_iter=100, n_jobs=-1)\n\nstratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Perform cross-validation using recall as the metric:\nvalidation_scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=stratified_kfold, scoring='recall', n_jobs=-1)\n\n# Print all 5 validation recall scores:\nprint(f\"Validation Recall Scores: {validation_scores}\")\n\n# As long as validation scores are efficient, proceed with the model training:\nmodel.fit(X_train, y_train)\n\n# Apply the trained model on unseen data to test its performance:\ny_pred = model.predict(X_test)\n\n# Stop the timer\nend_time = time.time()\n# Calculate the time taken\nelapsed_time_mins = round((end_time - start_time) / 60, 2)\n\n# Call a custom function to display the model prediction scores:\ndf_eval = get_classification_metrics(y_test=y_test, y_pred=y_pred, conf_matrix_title='Confusion Matrix of Canceled Bookings', runtime=elapsed_time_mins)\ndf_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:47.396601Z","iopub.execute_input":"2025-05-02T15:46:47.396911Z","iopub.status.idle":"2025-05-02T15:46:57.950269Z","shell.execute_reply.started":"2025-05-02T15:46:47.396889Z","shell.execute_reply":"2025-05-02T15:46:57.949215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.5. Results Interpretation","metadata":{}},{"cell_type":"code","source":"# Call a custom function to properly display the logistic regression results\ninterpretation_df = logreg_interpretation(X_train_scaled=X_train, model=model)\ninterpretation_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:57.951584Z","iopub.execute_input":"2025-05-02T15:46:57.95195Z","iopub.status.idle":"2025-05-02T15:46:57.968272Z","shell.execute_reply.started":"2025-05-02T15:46:57.951921Z","shell.execute_reply":"2025-05-02T15:46:57.96732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Random Forest Model","metadata":{}},{"cell_type":"markdown","source":"Random Forest needs exactly the same dataset with logreg, that is, **logreg_rf_data.csv**.  \n***************************************************************************************************************************************************\nFeatures and targets as well as train, test splits and scaling must be exactly the same with the baseline logistic regression model. Therefore, I proceed directly with model training, validation, predictions and evaluation.  \n\nRandom Forest is an advanced traditional machine learning technique capable of both regression and classification tasks, making it a suitable choice for comparison against the baseline logistic regression model. The goal is to outperform the logreg model by generating better predictions for cancellations. \n\nHere it runs smoothly without encountering any challenges. I did not use GridSearchCV for the same reasons mentioned earlier, specifically, to avoid increasing the runtime unnecessarily. The best RF model was achieved with the criterion set to ‘entropy’, as it provided slightly better results compared to ‘gini’, while the runtimes were nearly identical.  \n\nAdditionally, I set max_features=None after noticing that this setting improved the results without leading to overfitting. ","metadata":{}},{"cell_type":"markdown","source":"### 6.1. Train & Validate, Make Predictions and Display Evaluation Results","metadata":{}},{"cell_type":"code","source":"# Start the timer\nstart_time = time.time()\n\nmodel = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=30, max_features=None, random_state=42, n_jobs=-1, class_weight='balanced')\n\nstratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Perform cross-validation using recall as the metric:\nvalidation_scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=stratified_kfold, scoring='recall', n_jobs=-1)\n\n# Print all 5 validation recall scores:\nprint(f\"Validation Recall Scores: {validation_scores}\")\n\n# As long as validation scores are efficient, proceed with the model training:\nmodel.fit(X_train, y_train)\n\n# Apply the trained model on unseen data to test its performance:\ny_pred = model.predict(X_test)\n\n# Stop the timer\nend_time = time.time()\n# Calculate the time taken\nelapsed_time_mins = round((end_time - start_time) / 60, 2)\n\n# Call a custom function to display the model prediction scores:\ndf_eval = get_classification_metrics(y_test=y_test, y_pred=y_pred, conf_matrix_title='Confusion Matrix of Canceled Bookings', runtime=elapsed_time_mins)\ndf_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:46:57.969219Z","iopub.execute_input":"2025-05-02T15:46:57.969518Z","iopub.status.idle":"2025-05-02T15:48:23.657784Z","shell.execute_reply.started":"2025-05-02T15:46:57.969469Z","shell.execute_reply":"2025-05-02T15:48:23.656862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.5. Results Interpretation","metadata":{}},{"cell_type":"code","source":"# Call a custom function to properly display the RF results\ninterpretation_df = random_forest_interpretation(X_train_scaled=X_train, model=model)\ninterpretation_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:48:23.658755Z","iopub.execute_input":"2025-05-02T15:48:23.659329Z","iopub.status.idle":"2025-05-02T15:48:23.709051Z","shell.execute_reply.started":"2025-05-02T15:48:23.659305Z","shell.execute_reply":"2025-05-02T15:48:23.708017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Report to Stakeholders  \n**What the Models Say about Cancellations?**  \n\n• ADR AND LEAD TIME  \nBoth models showed that these are important features. I am satisfied with the Random Forest (RF) results, which highlight lead time and ADR as the most important features for predicting cancellations, as this makes logical sense. RF does not directly show feature direction or exact importance, but the fact that these variables contribute significantly to tree splits indicates a strong influence on predictions. We can also assume their relationship with cancellations is positive. First, this aligns with common sense, and second, it matches the corresponding positive odds in Logistic Regression (LogReg). In other words, a high ADR might cause a customer to cancel after finding a cheaper alternative, and a high lead time increases the likelihood of cancellation since plans often change over time.  \n\n• NUMBER OF PREVIOUS CANCELLATIONS  \nBoth models showed that the number of previous cancellations is also important (5th highest odds ratio in LogReg and 10th most important feature in RF). This is expected, as people who canceled in the past are more likely to cancel again.  \n\n• PEOPLE FROM PORTUGAL  \nPeople from Portugal are more likely to cancel, but I cannot interpret this confidently without feedback from the stakeholders. However, I assume the hotels are located in Portugal, as most customers are Portuguese. In that case, there’s a possible explanation: natives may be moreflexible knowing the local areas and having more flexible dates, while international travelers usually have fixed travel plans, making them less likely to cancel.  \n\n• ONLINE TRAVEL AGENCIES  \nBoth models showed a strong connection between OTA bookings and cancellations (2nd in LogReg odds and 4th in RF importance). We can also assume a positive direction here, based on both the models and logic. One explanation is that the online environment makes booking easier. Customers can book without paying upfront and compare many options at the same time. This might lead to spontaneous bookings, which are more likely to be canceled later.  \n***************************************************************************************************************************************************\n**Suggestions to the Stakeholders**  \n\n• ADR & ONLINE TRAVEL AGENCIES  \nHotels should use Property Management Systems (PMS) with dynamic pricing, uploading realtime price updates to OTAs. Dynamic pricing must adapt to market conditions, seasonal trends and customer behavior. A good Machine Learning model can support this in real-time by learning from historical data and ongoing booking trends. It can identify patterns where cancellations are more likely and suggest price changes to reduce risk. Also, modern PMS systems aim to reduce OTA dependency by increasing direct bookings, which usually bring higher margins and lower cancellation rates.  \n\n• LEAD TIME  \nHotels could apply motivation strategies like “book for the next month and get a discount or a gift” to encourage short-term bookings and reduce the risks of long lead times. For bookings made far in advance (e.g., more than one year), they could offer a small discount only if the booking is paid upfront. This way, both the hotel and the customer benefit from security and commitment.  \n\n• NUMBER OF PREVIOUS CANCELLATIONS  \nA solution here is to create personalized offers for users who canceled in the past. These could be combined with lead-time strategies, such as offering upfront-pay discounts for frequent cancelers, to reduce future risk.  \n\n• PORTUGUESE PEOPLE  \nThere is a mutual benefit on supporting the domestic market with discounted direct booking options targeted at Portuguese customers, especially during low-demand seasons. This builds loyalty and may reduce the overall cancellation rate from locals.","metadata":{}},{"cell_type":"markdown","source":"## Part 2: KPI Business Dashboard in Looker Studio for Strategic Planning Monitoring","metadata":{}},{"cell_type":"markdown","source":"The preprocessed file needed is the **dashboard_data.csv**.  \n***************************************************************************************************************************************************\nI calculate KPIs based on both part 1 and hospitality domain knowledge. For example, I include financial KPIs in addition to KPIs related to booking cancellations, to provide a complete picture of strategic KPIs to the stakeholders. I store the KPIs in two tabular structures (**hotel_market_segments.csv** and **hotel_kpis.csv**) locally on my machine without loading them into the company's db. \n\nSince my case study is about a static but fully updated KPI dashboard, the calculated fields needed in LS can be prepared outside of it using Python. In this way, the dashboard update times are minimized because LS only needs to reconnect to the updated data sources after each new data upload. After this short task, the static but fully updated KPI dashboard is ready to be shared with stakeholders.  \n\nFinally, the person who works with LS does not need to know low-level dashboard coding. They only need to follow simple, repeated steps in LS data management.","metadata":{}},{"cell_type":"code","source":"# Read the correct cleaned dataset for KPI creation\ndf = pd.read_csv(\"dashboard_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:48:23.710146Z","iopub.execute_input":"2025-05-02T15:48:23.710545Z","iopub.status.idle":"2025-05-02T15:48:24.025725Z","shell.execute_reply.started":"2025-05-02T15:48:23.71051Z","shell.execute_reply":"2025-05-02T15:48:24.024537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Create the KPI DataFrames and Export them to .csv Files","metadata":{}},{"cell_type":"code","source":"filter_city_hotel = df[df['hotel'] == 'City Hotel']\nfilter_resort_hotel = df[df['hotel'] == 'Resort Hotel']\n\ntotal_obs = df.shape[0]\ntotal_obs_city_hotel = df[df['hotel'] == 'City Hotel'].shape[0]\ntotal_obs_resort_hotel = df[df['hotel'] == 'Resort Hotel'].shape[0]\n\n# AVERAGE CANCELLATION RATE\navg_canc_rate = round(df['is_canceled'].mean() * 100, 2)\navg_canc_rate_city_hotel = round(filter_city_hotel['is_canceled'].mean() * 100, 2)\navg_canc_rate_resort_hotel = round(filter_resort_hotel['is_canceled'].mean() * 100, 2)\nprint(f\"Average Cancellation Rate: {avg_canc_rate} %\")\nprint(f\"Average Cancellation Rate City Hotel: {avg_canc_rate_city_hotel} %\")\nprint(f\"Average Cancellation Rate Resort Hotel: {avg_canc_rate_resort_hotel} %\")\n\n# AVERAGE PREVIOUS CANCELLATION RATE\navg_previous_canc_rate = round(df['previous_cancellations'].mean() * 100, 2)\navg_previous_canc_rate_city_hotel = round(filter_city_hotel['previous_cancellations'].mean() * 100, 2)\navg_previous_canc_rate_resort_hotel = round(filter_resort_hotel['previous_cancellations'].mean() * 100, 2)\nprint(f\"Average Previous Cancellation Rate: {avg_previous_canc_rate} %\")\nprint(f\"Average Previous Cancellation Rate City Hotel: {avg_previous_canc_rate_city_hotel} %\")\nprint(f\"Average Previous Cancellation Rate Resort Hotel: {avg_previous_canc_rate_resort_hotel} %\")\n\n\n# TOTAL REVENUE\ntotal_revenue = round(df['adr'].sum())\ntotal_revenue_city_hotel = round(filter_city_hotel['adr'].sum())\ntotal_revenue_resort_hotel = round(filter_resort_hotel['adr'].sum())\nprint(f\"Total Revenue: {total_revenue} €\")\nprint(f\"Total Revenue City Hotel: {total_revenue_city_hotel} €\")\nprint(f\"Total Revenue Resort Hotel: {total_revenue_resort_hotel} €\")\n\n# AVERAGE DAILY RATE\nadr = round(df['adr'].mean(), 2)\nadr_city_hotel = round(filter_city_hotel['adr'].mean(), 2)\nadr_resort_hotel = round(filter_resort_hotel['adr'].mean(), 2)\nprint(f\"Average Daily Rate: {adr} €\")\nprint(f\"Average Daily Rate City Hotel: {adr_city_hotel} €\")\nprint(f\"Average Daily Rate Resort Hotel: {adr_resort_hotel} €\")\n\n# AVERAGE LEAD TIME\navg_lead_time = round(df['lead_time'].mean(), 2)\navg_lead_time_city_hotel = round(filter_city_hotel['lead_time'].mean(), 2)\navg_lead_time_resort_hotel = round(filter_resort_hotel['lead_time'].mean(), 2)\nprint(f\"Average Lead Time: {avg_lead_time} days\")\nprint(f\"Average Lead Time City Hotel: {avg_lead_time_city_hotel} days\")\nprint(f\"Average Lead TIme Resort Hotel: {avg_lead_time_resort_hotel} days\")\n\n# REVENUE PER GUEST\ntotal_guests = df['adults'].sum() + df['total_kids'].sum()\ntotal_guests_city_hotel = filter_city_hotel['adults'].sum() + filter_city_hotel['total_kids'].sum()\ntotal_guests_resort_hotel = filter_resort_hotel['adults'].sum() + filter_resort_hotel['total_kids'].sum()\n\nrevpg = round(total_revenue / total_guests, 2)\nrevpg_city_hotel = round(total_revenue_city_hotel / total_guests_city_hotel, 2)\nrevpg_resort_hotel = round(total_revenue_resort_hotel / total_guests_resort_hotel, 2)\nprint(f\"Average Revenue per Guest: {revpg} €\")\nprint(f\"Average Revenue per Guest City Hotel: {revpg_city_hotel} €\")\nprint(f\"Average Revenue per Guest Resort Hotel: {revpg_resort_hotel} €\")\n\n# LENGTH OF STAY\ntotal_nights = df['stays_in_week_nights'].sum() + df['stays_in_weekend_nights'].sum()\ntotal_nights_city_hotel = filter_city_hotel['stays_in_week_nights'].sum() + filter_city_hotel['stays_in_weekend_nights'].sum()\ntotal_nights_resort_hotel = filter_resort_hotel['stays_in_week_nights'].sum() + filter_resort_hotel['stays_in_weekend_nights'].sum()\n\nlength_of_stay = round(total_nights / total_obs, 2)\nlength_of_stay_city_hotel = round(total_nights_city_hotel / total_obs_city_hotel, 2)\nlength_of_stay_resort_hotel = round(total_nights_resort_hotel / total_obs_resort_hotel, 2)\nprint(f\"Length of Stay: {length_of_stay} days\")\nprint(f\"Length of Stay City Hotel: {length_of_stay_city_hotel} days\")\nprint(f\"Length of Stay Resort Hotel: {length_of_stay_resort_hotel} days\")\n\n# BOOKINGS BY SOURCE (DATAFRAME)\nvalues = df['market_segment'].value_counts().values\nvalues_perc = np.round(100 * (values / total_obs), 2)\nvalues_city_hotel = df[df['hotel'] == 'City Hotel']['market_segment'].value_counts().values\nvalues_perc_city_hotel = np.round(100 * (values_city_hotel / total_obs_city_hotel), 2)\nvalues_resort_hotel = df[df['hotel'] == 'Resort Hotel']['market_segment'].value_counts().values\nvalues_perc_resort_hotel = np.round(100 * (values_resort_hotel / total_obs_resort_hotel), 2)\n\nindex = df['market_segment'].value_counts().keys()\ncolumns = ['Total Counts', 'Total Counts (%)','City Hotel Counts', 'City Hotel Counts (%)', 'Resort Hotel Counts', 'Resort Hotel Counts (%)']\ndata = list(zip(values, values_perc, values_city_hotel, values_perc_city_hotel, values_resort_hotel, values_perc_resort_hotel))\n\nmarket_df = pd.DataFrame(index=index, data=data, columns=columns)\nmarket_df.index.name = 'Market Segment'\nmarket_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:48:24.026885Z","iopub.execute_input":"2025-05-02T15:48:24.02736Z","iopub.status.idle":"2025-05-02T15:48:24.245793Z","shell.execute_reply.started":"2025-05-02T15:48:24.02732Z","shell.execute_reply":"2025-05-02T15:48:24.244747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KPIs Summary Table (One-row DataFrame)\nkpis = {\n    'Total Bookings': total_obs,\n    'Total Bookings City Hotel': total_obs_city_hotel,\n    'Total Bookings Resort Hotel': total_obs_resort_hotel,\n    'Cancellation Rate (%)': avg_canc_rate,\n    'Cancellation Rate City Hotel (%)': avg_canc_rate_city_hotel,\n    'Cancellation Rate Resort Hotel (%)': avg_canc_rate_resort_hotel,\n    'Previous Cancellation Rate (%)': avg_previous_canc_rate,\n    'Previous Cancellation Rate City Hotel (%)': avg_previous_canc_rate_city_hotel,\n    'Previous Cancellation Rate Resort Hotel (%)': avg_previous_canc_rate_resort_hotel,\n    'Total Revenue (€)': total_revenue,\n    'Total Revenue City Hotel (€)': total_revenue_city_hotel,\n    'Total Revenue Resort Hotel (€)': total_revenue_resort_hotel,\n    'ADR (€)': adr,\n    'ADR City Hotel (€)': adr_city_hotel,\n    'ADR Resort Hotel (€)': adr_resort_hotel,\n    'Average Lead Time (days)': avg_lead_time,\n    'Average Lead Time City Hotel (days)': avg_lead_time_city_hotel,\n    'Average Lead Time Resort Hotel (days)': avg_lead_time_resort_hotel,\n    'Revenue per Guest (€)': revpg,\n    'Revenue per Guest City Hotel (€)': revpg_city_hotel,\n    'Revenue per Guest Resort Hotel (€)': revpg_resort_hotel,\n    'Length of Stay (days)': length_of_stay,\n    'Length of Stay City Hotel (days)': length_of_stay_city_hotel,\n    'Length of Stay Resort Hotel (days)': length_of_stay_resort_hotel\n}\n\n# Convert dict to a dataframe\nkpis_df = pd.DataFrame([kpis])\n\n# Save to CSV\nkpis_df.to_csv('hotel_kpis.csv', index=False)\nmarket_df.to_csv('hotel_market_segments.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:48:24.246768Z","iopub.execute_input":"2025-05-02T15:48:24.247058Z","iopub.status.idle":"2025-05-02T15:48:24.258069Z","shell.execute_reply.started":"2025-05-02T15:48:24.247032Z","shell.execute_reply":"2025-05-02T15:48:24.257202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Display the Looker Studio Dashboard","metadata":{}},{"cell_type":"markdown","source":"Please follow the URL below which leads to the final KPI dashboard:  \n[Looker Studio KPI Dashboard](https://lookerstudio.google.com/reporting/8ee13cf9-54e6-41ac-823e-af0706cec66c)  ","metadata":{}}]}